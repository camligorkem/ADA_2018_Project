{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordLemPos Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gorkem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Import stopwords with nltk.\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using WordLemPos data, we will try to find topics of the dataset. What we do as a first step is to check one month data for one country and see if we can find some meaningful results.\n",
    "We will check October 2016, US data for this purpose. If our results seems meaningful, we will try with one year data and expand gradually from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read wordLemPos Data file \n",
    "data_folder='../sample_data/'\n",
    "fileName= '16-10-us.txt'\n",
    "wordLemPos_big = pd.read_table(data_folder+fileName, quoting=csv.QUOTE_NONE, encoding = \"ISO-8859-1\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordLemPos = wordLemPos_big.copy()\n",
    "numRows = len(wordLemPos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17564427, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14637197</td>\n",
       "      <td>4739839025</td>\n",
       "      <td>@@14637197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14637197</td>\n",
       "      <td>4739839026</td>\n",
       "      <td>&lt;p&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14637197</td>\n",
       "      <td>4739839027</td>\n",
       "      <td>NEW</td>\n",
       "      <td>new</td>\n",
       "      <td>np1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14637197</td>\n",
       "      <td>4739839028</td>\n",
       "      <td>YORK</td>\n",
       "      <td>york</td>\n",
       "      <td>np1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14637197</td>\n",
       "      <td>4739839029</td>\n",
       "      <td>(</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14637197</td>\n",
       "      <td>4739839030</td>\n",
       "      <td>AP</td>\n",
       "      <td>ap</td>\n",
       "      <td>np1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14637197</td>\n",
       "      <td>4739839031</td>\n",
       "      <td>)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14637197</td>\n",
       "      <td>4739839032</td>\n",
       "      <td>--</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jj_nn1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14637197</td>\n",
       "      <td>4739839033</td>\n",
       "      <td>Donald</td>\n",
       "      <td>donald</td>\n",
       "      <td>np1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14637197</td>\n",
       "      <td>4739839034</td>\n",
       "      <td>Trump</td>\n",
       "      <td>trump</td>\n",
       "      <td>nn1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14637197</td>\n",
       "      <td>4739839035</td>\n",
       "      <td>'s</td>\n",
       "      <td>'s</td>\n",
       "      <td>ge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14637197</td>\n",
       "      <td>4739839036</td>\n",
       "      <td>five-day</td>\n",
       "      <td>five-day</td>\n",
       "      <td>jj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14637197</td>\n",
       "      <td>4739839037</td>\n",
       "      <td>feud</td>\n",
       "      <td>feud</td>\n",
       "      <td>nn1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14637197</td>\n",
       "      <td>4739839038</td>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "      <td>iw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14637197</td>\n",
       "      <td>4739839039</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>at1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0           1           2         3       4\n",
       "0   14637197  4739839025  @@14637197       NaN      fo\n",
       "1   14637197  4739839026         <p>       NaN     NaN\n",
       "2   14637197  4739839027         NEW       new     np1\n",
       "3   14637197  4739839028        YORK      york     np1\n",
       "4   14637197  4739839029           (       NaN       (\n",
       "5   14637197  4739839030          AP        ap     np1\n",
       "6   14637197  4739839031           )       NaN       )\n",
       "7   14637197  4739839032          --       NaN  jj_nn1\n",
       "8   14637197  4739839033      Donald    donald     np1\n",
       "9   14637197  4739839034       Trump     trump     nn1\n",
       "10  14637197  4739839035          's        's      ge\n",
       "11  14637197  4739839036    five-day  five-day      jj\n",
       "12  14637197  4739839037        feud      feud     nn1\n",
       "13  14637197  4739839038        with      with      iw\n",
       "14  14637197  4739839039           a         a     at1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape and content of the data\n",
    "display(wordLemPos.shape)\n",
    "wordLemPos.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is useful for this data is the lemma-stemmed version of the words of each position. By using them we will skip the preprocessing part of the words lemmatization and stemming. Therefore, we want textID and lemma columns from the data. Also we drop the ones that are NA in lemma since it means they are either special char or numbers and don't have a root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14637197</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14637197</td>\n",
       "      <td>york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14637197</td>\n",
       "      <td>ap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14637197</td>\n",
       "      <td>donald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14637197</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14637197</td>\n",
       "      <td>'s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14637197</td>\n",
       "      <td>five-day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14637197</td>\n",
       "      <td>feud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14637197</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14637197</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      textID     lemma\n",
       "2   14637197       new\n",
       "3   14637197      york\n",
       "5   14637197        ap\n",
       "8   14637197    donald\n",
       "9   14637197     trump\n",
       "10  14637197        's\n",
       "11  14637197  five-day\n",
       "12  14637197      feud\n",
       "13  14637197      with\n",
       "14  14637197         a"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wordLemPos.rename(columns={0:'textID',1:'ID(seq)',2:'word',3:'lemma',4:'PoS'},inplace=True)\n",
    "wordLemPos = wordLemPos[['textID','lemma']]\n",
    "# Drop NA values we are not interested in null lemma's since \n",
    "# they are not words but special characters and numbers\n",
    "wordLemPos.dropna(inplace=True)\n",
    "wordLemPos.shape\n",
    "display(wordLemPos.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the WordLemPos data for each news article words are already preprocessed and lemmatized and stemmed. What we need to do first is to get rid off the stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordLemPos = wordLemPos[:numRows]\n",
    "#display(wordLemPos.shape)\n",
    "#display(wordLemPos.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove stopwords we have several conditions the word shouldn't be nltk or gensim.parsing stopword dictionary, it shouldn't be in unnecessary words and the length of the word should be bigger than 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some unnecessary words found common but not useful for the topic modelling, we excluded them\n",
    "unnecessary_words=['new','good','high','big','with','into','under',\n",
    "                  'really','already','still','early','while','although','most','every','which',\n",
    "                  'year','like','time','that','given','would']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6135938, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14637197</td>\n",
       "      <td>york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14637197</td>\n",
       "      <td>donald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14637197</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14637197</td>\n",
       "      <td>five-day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14637197</td>\n",
       "      <td>feud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14637197</td>\n",
       "      <td>beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14637197</td>\n",
       "      <td>queen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>14637197</td>\n",
       "      <td>late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14637197</td>\n",
       "      <td>example</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>14637197</td>\n",
       "      <td>insistence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      textID       lemma\n",
       "3   14637197        york\n",
       "8   14637197      donald\n",
       "9   14637197       trump\n",
       "11  14637197    five-day\n",
       "12  14637197        feud\n",
       "16  14637197      beauty\n",
       "17  14637197       queen\n",
       "21  14637197        late\n",
       "22  14637197     example\n",
       "25  14637197  insistence"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find stop words, replace with nullin lemma\n",
    "wordLemPos['lemma'] = wordLemPos.lemma.apply((lambda x: x if x not in (stop) \n",
    "                                              and len(x)>3 \n",
    "                                              and x not in gensim.parsing.preprocessing.STOPWORDS\n",
    "                                              and x not in unnecessary_words\n",
    "                                              else None))\n",
    "# Drop lemma null - which are the stopwords\n",
    "wordLemPos.dropna(inplace=True)\n",
    "display(wordLemPos.shape)\n",
    "display(wordLemPos.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering the stopwords, we will make the words ready for the LDA.\n",
    "For each unique textID we group the words and turn them into list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of words for each article\n",
    "unique_textID = wordLemPos.textID.unique()\n",
    "docs =[]\n",
    "for text_id in unique_textID:\n",
    "    doc = wordLemPos[wordLemPos.textID==text_id]['lemma'].tolist()\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26028"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many article we have\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create bag of words. Assign a number key for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30-day\n",
      "1 airing\n",
      "2 article\n",
      "3 beauty\n",
      "4 brag\n",
      "5 brash\n",
      "6 businessman\n",
      "7 campaign\n",
      "8 celebrity\n",
      "9 cheer\n",
      "10 combat\n"
     ]
    }
   ],
   "source": [
    "# bag of words on the dataset\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another good approach is to filter the words there are not very common on the article to ease the further steps and computation. If word count is very few within the given documents then it is probably not related to find the topic of the article, so we can clear them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the tokens that are seen less than x documents\n",
    "docThreshold = 15\n",
    "dictionary.filter_extremes(no_below=docThreshold, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create doc2bow which is for each document we count the words they have and store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create doc2bow\n",
    "# For each doc have a dictionary stating how many words and how many times each word appears\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "#bow_corpus[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually need also bow_doc results for our further analysis as well. Therefore, we will save it to use for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 11 (\"come\") appears 3 time.\n",
      "Word 15 (\"cost\") appears 2 time.\n",
      "Word 19 (\"example\") appears 1 time.\n",
      "Word 29 (\"local\") appears 1 time.\n",
      "Word 32 (\"need\") appears 1 time.\n",
      "Word 37 (\"people\") appears 1 time.\n",
      "Word 38 (\"period\") appears 1 time.\n",
      "Word 39 (\"political\") appears 1 time.\n",
      "Word 45 (\"service\") appears 3 time.\n",
      "Word 56 (\"unfairly\") appears 1 time.\n",
      "Word 68 (\"basically\") appears 1 time.\n",
      "Word 78 (\"change\") appears 1 time.\n",
      "Word 108 (\"gain\") appears 1 time.\n",
      "Word 115 (\"happen\") appears 1 time.\n",
      "Word 123 (\"instead\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# Check 10th document which word appears how many time, print previous results\n",
    "bow_doc_10 = bow_corpus[10]\n",
    "#for i in range(len(bow_doc_10)):\n",
    "# check first 15 words\n",
    "for i in range(15):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_10[i][0], \n",
    "                                               dictionary[bow_doc_10[i][0]],bow_doc_10[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to file\n",
    "bow_corp = pd.DataFrame(bow_corpus)#, columns=['topic_id','words'])\n",
    "#topics.to_csv('results/topics'+fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.13875160249185722),\n",
      " (1, 0.17715031430766318),\n",
      " (2, 0.27721028525058744),\n",
      " (3, 0.12131747497512263),\n",
      " (4, 0.12347796102459174),\n",
      " (5, 0.1891044232255576),\n",
      " (6, 0.13041958050997313),\n",
      " (7, 0.062166725248688805),\n",
      " (8, 0.11560367908079414),\n",
      " (9, 0.12807161883869791),\n",
      " (10, 0.11848732068045718),\n",
      " (11, 0.025903339805993027),\n",
      " (12, 0.2098881712441338),\n",
      " (13, 0.07310700483804931),\n",
      " (14, 0.04553721662579709),\n",
      " (15, 0.0701098956552853),\n",
      " (16, 0.06377944504270776),\n",
      " (17, 0.08063285966758814),\n",
      " (18, 0.26822299077423606),\n",
      " (19, 0.07583102922564613),\n",
      " (20, 0.15614468491195388),\n",
      " (21, 0.20544178170293612),\n",
      " (22, 0.10073535252794036),\n",
      " (23, 0.12570775136353204),\n",
      " (24, 0.16872380372463114),\n",
      " (25, 0.038929328098851024),\n",
      " (26, 0.05031869614151054),\n",
      " (27, 0.17819931852133633),\n",
      " (28, 0.050998930179784505),\n",
      " (29, 0.06073026832316018),\n",
      " (30, 0.06649222693253704),\n",
      " (31, 0.05082413449427594),\n",
      " (32, 0.1494953527134458),\n",
      " (33, 0.04375062855431459),\n",
      " (34, 0.08233282954234428),\n",
      " (35, 0.05537289918246951),\n",
      " (36, 0.21395666957111653),\n",
      " (37, 0.02826045425081141),\n",
      " (38, 0.07388605736031423),\n",
      " (39, 0.07014770790032673),\n",
      " (40, 0.08706930972970299),\n",
      " (41, 0.12184261576733767),\n",
      " (42, 0.06341762854787839),\n",
      " (43, 0.06922917575957259),\n",
      " (44, 0.0710313579747438),\n",
      " (45, 0.15348518392513374),\n",
      " (46, 0.06595285712277416),\n",
      " (47, 0.10405090961109652),\n",
      " (48, 0.21796972211053503),\n",
      " (49, 0.08882224494857036),\n",
      " (50, 0.20544178170293612),\n",
      " (51, 0.09369344977955193),\n",
      " (52, 0.19613156499066747),\n",
      " (53, 0.13565816557001686),\n",
      " (54, 0.08339110012898177),\n",
      " (55, 0.06091282865912503),\n",
      " (56, 0.17112889927354658),\n",
      " (57, 0.10077179528300287),\n",
      " (58, 0.15767660791142038),\n",
      " (59, 0.1309754028548771),\n",
      " (60, 0.036749024503569656),\n",
      " (61, 0.062300930248229684)]\n"
     ]
    }
   ],
   "source": [
    "# TF IDF scores\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "# print first 10 tfidf scores\n",
    "for doc in corpus_tfidf[:10]:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the last step, we create the LDA model to find the topics. We precised 10 topics for now and do 4 passes over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the lda model with gensim providing bow_corpus and dicitonary we created\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=7, id2word=dictionary, passes=4, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.008*\"school\" + 0.008*\"city\" + 0.007*\"state\" + 0.006*\"student\" + 0.006*\"work\" + 0.005*\"need\" + 0.004*\"people\" + 0.004*\"community\" + 0.004*\"county\" + 0.004*\"service\"\n",
      "Topic: 1 \n",
      "Words: 0.018*\"police\" + 0.009*\"county\" + 0.008*\"officer\" + 0.007*\"charge\" + 0.006*\"report\" + 0.006*\"case\" + 0.006*\"court\" + 0.006*\"tell\" + 0.005*\"department\" + 0.005*\"according\"\n",
      "Topic: 2 \n",
      "Words: 0.019*\"company\" + 0.012*\"share\" + 0.009*\"million\" + 0.008*\"market\" + 0.008*\"stock\" + 0.008*\"price\" + 0.007*\"business\" + 0.007*\"percent\" + 0.007*\"report\" + 0.007*\"quarter\"\n",
      "Topic: 3 \n",
      "Words: 0.023*\"trump\" + 0.014*\"clinton\" + 0.009*\"election\" + 0.009*\"state\" + 0.008*\"campaign\" + 0.007*\"republican\" + 0.006*\"people\" + 0.006*\"vote\" + 0.006*\"voter\" + 0.005*\"president\"\n",
      "Topic: 4 \n",
      "Words: 0.007*\"people\" + 0.005*\"family\" + 0.005*\"know\" + 0.005*\"come\" + 0.004*\"life\" + 0.004*\"want\" + 0.004*\"work\" + 0.004*\"home\" + 0.003*\"think\" + 0.003*\"love\"\n",
      "Topic: 5 \n",
      "Words: 0.025*\"game\" + 0.014*\"play\" + 0.013*\"team\" + 0.011*\"season\" + 0.007*\"player\" + 0.006*\"yard\" + 0.006*\"week\" + 0.006*\"come\" + 0.005*\"second\" + 0.005*\"field\"\n",
      "Topic: 6 \n",
      "Words: 0.005*\"people\" + 0.005*\"work\" + 0.004*\"know\" + 0.004*\"study\" + 0.004*\"world\" + 0.004*\"look\" + 0.004*\"thing\" + 0.003*\"come\" + 0.003*\"think\" + 0.003*\"need\"\n"
     ]
    }
   ],
   "source": [
    "# Print out the find topics most common words\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to file\n",
    "topics = pd.DataFrame(lda_model.print_topics(-1), columns=['topic_id','words'])\n",
    "topics.to_csv('../results/topics'+fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above topics most common words, we can actually come up with good idea for news topic. For example:\n",
    "- Topic 0: education\n",
    "- Topic 1: crime\n",
    "- Topic 2: business / economy\n",
    "- Topic 3: politics / election\n",
    "- Topic 4: social / life\n",
    "- Topic 5: sports\n",
    "- Topic 6: world/ daily / ?\n",
    "\n",
    "Looks like we can find some good topics. We will proceed with this approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
